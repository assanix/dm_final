{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline\n",
    "## Unified approach combining all best practices from drafts\n",
    "\n",
    "**Sources:**\n",
    "- Basic RFM features from `baseline.ipynb` (23 features)\n",
    "- Periodic aggregations from `ozon-fresh-categories_NS.ipynb` (277 features)\n",
    "- UMAP embeddings from `categoricalembeddinglowdim.ipynb` (27 features)\n",
    "- Temporal patterns from `features_from_pdf.ipynb`\n",
    "- Advanced features (new)\n",
    "\n",
    "**Target:** ~70-80 features initially ‚Üí expand if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import glob\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For UMAP embeddings (optional - can add later)\n",
    "# from umap import UMAP\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2024-03-01 to 2024-06-30\n",
      "Validation period: 2024-07-01 to 2024-07-31\n",
      "Test prediction: 2024-08-01\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_PATH = '../docs'  # Your loaded data\n",
    "\n",
    "# Date ranges\n",
    "TRAIN_START_DATE = pd.Timestamp('2024-03-01')   # 5 months of history for training\n",
    "TRAIN_END_DATE = pd.Timestamp('2024-06-30')\n",
    "VAL_START_DATE = pd.Timestamp('2024-07-01')\n",
    "VAL_END_DATE = pd.Timestamp('2024-07-31')\n",
    "TEST_START_DATE = pd.Timestamp('2024-08-01')\n",
    "NUM_PERIODS = 4 \n",
    "\n",
    "print(f\"Train period: {TRAIN_START_DATE.date()} to {TRAIN_END_DATE.date()}\")\n",
    "print(f\"Validation period: {VAL_START_DATE.date()} to {VAL_END_DATE.date()}\")\n",
    "print(f\"Test prediction: {TEST_START_DATE.date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading actions_history...\n",
      "Found 53 action files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading actions:   0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading actions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53/53 [00:03<00:00, 14.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions history shape: (182001544, 6)\n",
      "Memory usage: 5207.11 MB\n",
      "\n",
      "Date range: 2011-05-28 00:26:26 to 2024-07-31 23:59:58\n",
      "Unique users: 5,224,053\n",
      "Unique products: 374,821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load actions_history (multiple parquet files)\n",
    "import glob  \n",
    "from tqdm import tqdm  \n",
    "import gc \n",
    "print(\"Loading actions_history...\")\n",
    "actions_files = sorted(glob.glob(os.path.join(DATA_PATH, 'actions_history', '*.parquet')))\n",
    "print(f\"Found {len(actions_files)} action files\")\n",
    "\n",
    "if len(actions_files) == 0:\n",
    "    raise FileNotFoundError(f\"No parquet files found in {os.path.join(DATA_PATH, 'actions_history')}\")\n",
    "\n",
    "actions_list = []\n",
    "for file in tqdm(actions_files, desc=\"Loading actions\"):\n",
    "    try:\n",
    "        df = pd.read_parquet(file)\n",
    "        # Convert timestamp to datetime\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "        actions_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "        raise\n",
    "\n",
    "actions_history = pd.concat(actions_list, ignore_index=True)\n",
    "print(f\"Actions history shape: {actions_history.shape}\")\n",
    "print(f\"Memory usage: {actions_history.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDate range: {actions_history['timestamp'].min()} to {actions_history['timestamp'].max()}\")\n",
    "print(f\"Unique users: {actions_history['user_id'].nunique():,}\")\n",
    "print(f\"Unique products: {actions_history['product_id'].nunique():,}\")\n",
    "\n",
    "del actions_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading search_history...\n",
      "Found 32 search files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading searches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:08<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search history shape: (78160845, 5)\n",
      "Memory usage: 7893.96 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load search_history (multiple parquet files)\n",
    "print(\"Loading search_history...\")\n",
    "search_files = sorted(glob.glob(os.path.join(DATA_PATH, 'search_history', '*.parquet')))\n",
    "print(f\"Found {len(search_files)} search files\")\n",
    "\n",
    "search_list = []\n",
    "for file in tqdm(search_files, desc=\"Loading searches\"):\n",
    "    df = pd.read_parquet(file)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    search_list.append(df)\n",
    "\n",
    "search_history = pd.concat(search_list, ignore_index=True)\n",
    "print(f\"Search history shape: {search_history.shape}\")\n",
    "print(f\"Memory usage: {search_history.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "del search_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading product information...\n",
      "Product information shape: (238443, 8)\n",
      "Test users shape: (2068424, 1)\n",
      "\n",
      "Sample of actions_history:\n",
      "    user_id           timestamp  product_id  page_product_id  action_type_id  \\\n",
      "0   7158706 2024-03-02 13:14:27   162625954              NaN               5   \n",
      "1   2762233 2024-05-28 14:20:44   148481523              NaN               5   \n",
      "2   6415797 2024-04-28 18:18:30   371796916              NaN               5   \n",
      "3  11178472 2024-06-08 10:35:43   887739173              NaN               5   \n",
      "4   2695403 2024-04-12 11:14:52   163600519              NaN               5   \n",
      "\n",
      "   widget_name_id  \n",
      "0              22  \n",
      "1              22  \n",
      "2              22  \n",
      "3              22  \n",
      "4              22  \n",
      "\n",
      "Action types:\n",
      "action_type_id\n",
      "1    66968540\n",
      "2     4065805\n",
      "3    31306914\n",
      "5    79660285\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load product information and other metadata\n",
    "print(\"Loading product information...\")\n",
    "product_information = pd.read_csv(os.path.join(DATA_PATH, 'product_information.csv'))\n",
    "print(f\"Product information shape: {product_information.shape}\")\n",
    "\n",
    "# Test users\n",
    "test_users = pd.read_csv(os.path.join(DATA_PATH, 'test_users.csv'))\n",
    "print(f\"Test users shape: {test_users.shape}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of actions_history:\")\n",
    "print(actions_history.head())\n",
    "print(\"\\nAction types:\")\n",
    "print(actions_history['action_type_id'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation target...\n",
      "\n",
      "Total users: 1,835,147\n",
      "\n",
      "Target distribution:\n",
      "target\n",
      "0    1200425\n",
      "1     634722\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Positive class ratio: 34.59%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation target (for training)\n",
    "# Users who made an order (action_type_id == 3) in July 2024\n",
    "print(\"Creating validation target...\")\n",
    "\n",
    "val_actions = actions_history[\n",
    "    (actions_history['timestamp'] >= VAL_START_DATE) &\n",
    "    (actions_history['timestamp'] <= VAL_END_DATE)\n",
    "].copy()\n",
    "\n",
    "val_target = (\n",
    "    val_actions\n",
    "    .assign(has_order=(val_actions['action_type_id'] == 3).astype(int))\n",
    "    .groupby('user_id', as_index=False)\n",
    "    .agg(target=('has_order', 'max'))\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal users: {val_target.shape[0]:,}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(val_target['target'].value_counts())\n",
    "\n",
    "# Calculate class imbalance\n",
    "positive_ratio = val_target['target'].mean()\n",
    "print(f\"\\nPositive class ratio: {positive_ratio:.2%}\")\n",
    "\n",
    "del val_actions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Generation Functions\n",
    "### 4.1 Basic RFM Features (from baseline.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_basic_rfm_features(\n",
    "    user_df: pd.DataFrame,\n",
    "    start_date: pd.Timestamp,\n",
    "    end_date: pd.Timestamp\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate basic RFM features for each action type.\n",
    "    Based on baseline.ipynb approach.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with ~23 basic features per action type\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Generating Basic RFM Features ===\")\n",
    "    \n",
    "    df = user_df.copy()\n",
    "    \n",
    "    actions_id_to_suf = {\n",
    "        1: \"click\",\n",
    "        2: \"favorite\",\n",
    "        3: \"order\",\n",
    "        5: \"to_cart\",\n",
    "    }\n",
    "    \n",
    "    # Filter actions for the period\n",
    "    period_actions = actions_history[\n",
    "        (actions_history['timestamp'] >= start_date) &\n",
    "        (actions_history['timestamp'] <= end_date)\n",
    "    ].copy()\n",
    "    \n",
    "    # Merge with product info for prices\n",
    "    period_actions = period_actions.merge(\n",
    "        product_information[['product_id', 'discount_price']],\n",
    "        on='product_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    for action_id, suffix in actions_id_to_suf.items():\n",
    "        print(f\"  Processing {suffix}s...\")\n",
    "        \n",
    "        action_data = period_actions[period_actions['action_type_id'] == action_id].copy()\n",
    "        \n",
    "        if len(action_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Aggregate by user\n",
    "        aggs = action_data.groupby('user_id').agg(\n",
    "            **{\n",
    "                f'num_products_{suffix}': ('product_id', 'count'),\n",
    "                f'num_unique_products_{suffix}': ('product_id', 'nunique'),\n",
    "                f'sum_discount_price_{suffix}': ('discount_price', 'sum'),\n",
    "                f'max_discount_price_{suffix}': ('discount_price', 'max'),\n",
    "                f'last_{suffix}_time': ('timestamp', 'max'),\n",
    "                f'first_{suffix}_time': ('timestamp', 'min'),\n",
    "            }\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Calculate recency features\n",
    "        reference_date = end_date + timedelta(days=1)\n",
    "        aggs[f'days_since_last_{suffix}'] = (reference_date - aggs[f'last_{suffix}_time']).dt.days\n",
    "        aggs[f'days_since_first_{suffix}'] = (reference_date - aggs[f'first_{suffix}_time']).dt.days\n",
    "        \n",
    "        # Drop timestamp columns\n",
    "        aggs = aggs.drop(columns=[f'last_{suffix}_time', f'first_{suffix}_time'])\n",
    "        \n",
    "        # Merge with main dataframe\n",
    "        df = df.merge(aggs, on='user_id', how='left')\n",
    "    \n",
    "    # Search aggregations\n",
    "    print(\"  Processing searches...\")\n",
    "    suffix = 'search'\n",
    "    \n",
    "    period_searches = search_history[\n",
    "        (search_history['timestamp'] >= start_date) &\n",
    "        (search_history['timestamp'] <= end_date)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(period_searches) > 0:\n",
    "        search_aggs = period_searches.groupby('user_id').agg(\n",
    "            **{\n",
    "                f'num_{suffix}': ('search_query', 'count'),\n",
    "                f'last_{suffix}_time': ('timestamp', 'max'),\n",
    "                f'first_{suffix}_time': ('timestamp', 'min'),\n",
    "            }\n",
    "        ).reset_index()\n",
    "        \n",
    "        reference_date = end_date + timedelta(days=1)\n",
    "        search_aggs[f'days_since_last_{suffix}'] = (reference_date - search_aggs[f'last_{suffix}_time']).dt.days\n",
    "        search_aggs[f'days_since_first_{suffix}'] = (reference_date - search_aggs[f'first_{suffix}_time']).dt.days\n",
    "        \n",
    "        search_aggs = search_aggs.drop(columns=[f'last_{suffix}_time', f'first_{suffix}_time'])\n",
    "        \n",
    "        df = df.merge(search_aggs, on='user_id', how='left')\n",
    "    \n",
    "    # Count generated features\n",
    "    new_features = len(df.columns) - len(user_df.columns)\n",
    "    print(f\"  Generated {new_features} RFM features\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Temporal Features (from features_from_pdf.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temporal_features(\n",
    "    user_df: pd.DataFrame,\n",
    "    start_date: pd.Timestamp,\n",
    "    end_date: pd.Timestamp\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate temporal pattern features:\n",
    "    - Favorite day of week\n",
    "    - Average hour of activity\n",
    "    - Number of unique active days\n",
    "    - Lifecycle features (is_new_user, lifetime)\n",
    "    \n",
    "    From features_from_pdf.ipynb\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Generating Temporal Features ===\")\n",
    "    \n",
    "    df = user_df.copy()\n",
    "    \n",
    "    actions_id_to_suf = {\n",
    "        1: \"click\",\n",
    "        2: \"favorite\",\n",
    "        3: \"order\",\n",
    "        5: \"to_cart\",\n",
    "    }\n",
    "    \n",
    "    period_actions = actions_history[\n",
    "        (actions_history['timestamp'] >= start_date) &\n",
    "        (actions_history['timestamp'] <= end_date)\n",
    "    ].copy()\n",
    "    \n",
    "    # Add temporal columns\n",
    "    period_actions['day_of_week'] = period_actions['timestamp'].dt.dayofweek\n",
    "    period_actions['hour'] = period_actions['timestamp'].dt.hour\n",
    "    period_actions['date'] = period_actions['timestamp'].dt.date\n",
    "    \n",
    "    for action_id, suffix in actions_id_to_suf.items():\n",
    "        print(f\"  Processing {suffix} temporal patterns...\")\n",
    "        \n",
    "        action_data = period_actions[period_actions['action_type_id'] == action_id].copy()\n",
    "        \n",
    "        if len(action_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        temporal_aggs = action_data.groupby('user_id').agg(\n",
    "            **{\n",
    "                f'favorite_day_of_week_{suffix}': ('day_of_week', 'mean'),\n",
    "                f'avg_hour_{suffix}': ('hour', 'mean'),\n",
    "                f'num_unique_days_{suffix}': ('date', 'nunique'),\n",
    "                f'first_time_{suffix}': ('timestamp', 'min'),\n",
    "            }\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Is new user (started after June 1st)\n",
    "        temporal_aggs[f'is_new_user_{suffix}'] = (\n",
    "            temporal_aggs[f'first_time_{suffix}'] >= pd.Timestamp('2024-06-01')\n",
    "        ).astype(int)\n",
    "        \n",
    "        temporal_aggs = temporal_aggs.drop(columns=[f'first_time_{suffix}'])\n",
    "        \n",
    "        df = df.merge(temporal_aggs, on='user_id', how='left')\n",
    "    \n",
    "    # Add lifetime features (days between first and last activity)\n",
    "    for suffix in ['click', 'favorite', 'order', 'to_cart']:\n",
    "        first_col = f'days_since_first_{suffix}'\n",
    "        last_col = f'days_since_last_{suffix}'\n",
    "        \n",
    "        if first_col in df.columns and last_col in df.columns:\n",
    "            df[f'lifetime_{suffix}'] = df[first_col] - df[last_col]\n",
    "    \n",
    "    print(f\"  Generated temporal features\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Conversion Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conversion_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate conversion rate features:\n",
    "    - click_to_order_conversion\n",
    "    - favorite_to_order_conversion\n",
    "    - to_cart_to_order_conversion\n",
    "    - searches_to_order_ratio\n",
    "    - actions_per_day\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Generating Conversion Features ===\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Conversion rates\n",
    "    for suffix in ['click', 'favorite', 'to_cart']:\n",
    "        num_col = f'num_products_{suffix}'\n",
    "        if num_col in df.columns and 'num_products_order' in df.columns:\n",
    "            df[f'{suffix}_to_order_conversion'] = (\n",
    "                df['num_products_order'] / df[num_col].replace(0, np.nan)\n",
    "            )\n",
    "    \n",
    "    # Search to order ratio\n",
    "    if 'num_search' in df.columns and 'num_products_order' in df.columns:\n",
    "        df['searches_to_order_ratio'] = (\n",
    "            df['num_search'] / df['num_products_order'].replace(0, np.nan)\n",
    "        )\n",
    "    \n",
    "    # Actions per day\n",
    "    for suffix in ['click', 'favorite', 'to_cart', 'order']:\n",
    "        num_col = f'num_unique_products_{suffix}'\n",
    "        days_col = f'num_unique_days_{suffix}'\n",
    "        \n",
    "        if num_col in df.columns and days_col in df.columns:\n",
    "            df[f'{suffix}_per_day'] = (\n",
    "                df[num_col] / df[days_col].replace(0, np.nan)\n",
    "            )\n",
    "    \n",
    "    print(\"  Generated conversion features\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Advanced Features (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_advanced_features(\n",
    "    user_df: pd.DataFrame,\n",
    "    start_date: pd.Timestamp,\n",
    "    end_date: pd.Timestamp\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate advanced behavioral features:\n",
    "    - Discount purchase ratio\n",
    "    - Category diversity\n",
    "    - Widget diversity\n",
    "    - Price sensitivity\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Generating Advanced Features ===\")\n",
    "    \n",
    "    df = user_df.copy()\n",
    "    \n",
    "    period_actions = actions_history[\n",
    "        (actions_history['timestamp'] >= start_date) &\n",
    "        (actions_history['timestamp'] <= end_date)\n",
    "    ].copy()\n",
    "    \n",
    "    # 1. Discount purchase ratio\n",
    "    print(\"  Calculating discount ratios...\")\n",
    "    order_actions = period_actions[period_actions['action_type_id'] == 3].copy()\n",
    "    \n",
    "    if len(order_actions) > 0:\n",
    "        order_actions = order_actions.merge(\n",
    "            product_information[['product_id', 'price', 'discount_price']],\n",
    "            on='product_id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        order_actions['has_discount'] = (\n",
    "            order_actions['price'] > order_actions['discount_price']\n",
    "        ).astype(int)\n",
    "        \n",
    "        discount_aggs = order_actions.groupby('user_id').agg(\n",
    "            discount_purchase_ratio=('has_discount', 'mean'),\n",
    "            avg_order_price=('discount_price', 'mean')\n",
    "        ).reset_index()\n",
    "        \n",
    "        df = df.merge(discount_aggs, on='user_id', how='left')\n",
    "    \n",
    "    # 2. Category diversity\n",
    "    print(\"  Calculating category diversity...\")\n",
    "    interaction_actions = period_actions[\n",
    "        period_actions['action_type_id'].isin([1, 2, 3, 5])\n",
    "    ].copy()\n",
    "    \n",
    "    if len(interaction_actions) > 0:\n",
    "        interaction_actions = interaction_actions.merge(\n",
    "            product_information[['product_id', 'category_id']],\n",
    "            on='product_id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        category_aggs = interaction_actions.groupby('user_id').agg(\n",
    "            num_unique_categories=('category_id', 'nunique'),\n",
    "            total_interactions=('category_id', 'count')\n",
    "        ).reset_index()\n",
    "        \n",
    "        category_aggs['category_diversity'] = (\n",
    "            category_aggs['num_unique_categories'] / \n",
    "            category_aggs['total_interactions']\n",
    "        )\n",
    "        \n",
    "        category_aggs = category_aggs.drop(columns=['total_interactions'])\n",
    "        df = df.merge(category_aggs, on='user_id', how='left')\n",
    "    \n",
    "    # 3. Widget diversity\n",
    "    print(\"  Calculating widget diversity...\")\n",
    "    widget_aggs = period_actions.groupby('user_id').agg(\n",
    "        num_unique_widgets=('widget_name_id', 'nunique')\n",
    "    ).reset_index()\n",
    "    \n",
    "    df = df.merge(widget_aggs, on='user_id', how='left')\n",
    "    \n",
    "    print(\"  Generated advanced features\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_periodic_aggregations(\n",
    "    user_df: pd.DataFrame,\n",
    "    start_date: pd.Timestamp,\n",
    "    end_date: pd.Timestamp,\n",
    "    num_periods: int = 4\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –ü–µ—Ä–∏–æ–¥–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∏–∑ NS notebook.\n",
    "    –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ –ø–µ—Ä–∏–æ–¥—ã (4 –Ω–µ–¥–µ–ª–∏ + —Å—Ç–∞—Ä—à–µ) –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ.\n",
    "    –≠—Ç–æ –ª–æ–≤–∏—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏–Ω–∞–º–∏–∫—É - –Ω–µ–¥–∞–≤–Ω–µ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ vs –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–µ.\n",
    "    \n",
    "    –ü–µ—Ä–∏–æ–¥—ã:\n",
    "    - 0: –ü–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π\n",
    "    - 1: 8-14 –¥–Ω–µ–π –Ω–∞–∑–∞–¥\n",
    "    - 2: 15-21 –¥–µ–Ω—å –Ω–∞–∑–∞–¥\n",
    "    - 3: 22-28 –¥–Ω–µ–π –Ω–∞–∑–∞–¥\n",
    "    - 4: >28 –¥–Ω–µ–π (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ –ø–æ –¥–ª–∏–Ω–µ)\n",
    "    \n",
    "    –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä–∏–æ–¥-–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-–¥–µ–π—Å—Ç–≤–∏–µ:\n",
    "    - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π, –ø—Ä–æ–¥—É–∫—Ç–æ–≤, –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –≤–∏–¥–∂–µ—Ç–æ–≤\n",
    "    - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ü–µ–Ω (mean, max, min)\n",
    "    - Std timestamps (–∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏)\n",
    "    - –°–∞–º–∞—è —á–∞—Å—Ç–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è (–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–∞—è —Ñ–∏—á–∞)\n",
    "    \n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç ~200 —Ñ–∏—á–µ–π\n",
    "    \"\"\"\n",
    "    print(\"\\n=== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–∏–æ–¥–Ω—ã—Ö –∞–≥—Ä–µ–≥–∞—Ü–∏–π ===\")\n",
    "    print(f\"  –ü–µ—Ä–∏–æ–¥—ã: {num_periods} –Ω–µ–¥–µ–ª—å + —Å—Ç–∞—Ä—à–µ\")\n",
    "    \n",
    "    df = user_df.copy()\n",
    "    \n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º actions\n",
    "    period_actions = actions_history[\n",
    "        (actions_history['timestamp'] >= start_date) &\n",
    "        (actions_history['timestamp'] <= end_date) &\n",
    "        (actions_history['user_id'].isin(user_df['user_id']))\n",
    "    ].copy()\n",
    "    \n",
    "    if len(period_actions) == 0:\n",
    "        print(\"  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö\")\n",
    "        return df\n",
    "    \n",
    "    # –ú–µ—Ä–∂–∏–º —Å product_information\n",
    "    period_actions = period_actions.merge(\n",
    "        product_information[['product_id', 'category_id', 'price', 'discount_price']],\n",
    "        on='product_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
    "    period_actions['category_id'] = period_actions['category_id'].fillna(10000).astype(int)\n",
    "    period_actions['price'] = period_actions['price'].fillna(period_actions['price'].mean())\n",
    "    period_actions['discount_price'] = period_actions['discount_price'].fillna(\n",
    "        period_actions['discount_price'].mean()\n",
    "    )\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–∏–æ–¥ (0 = –ø–æ—Å–ª–µ–¥–Ω—è—è –Ω–µ–¥–µ–ª—è, 4 = —Å—Ç–∞—Ä—à–µ)\n",
    "    period_actions['period'] = ((end_date - period_actions['timestamp']).dt.days // 7).clip(upper=num_periods)\n",
    "    \n",
    "    # Timestamp –∫–∞–∫ integer –¥–ª—è std\n",
    "    period_actions['timestamp_int'] = (period_actions['timestamp'].astype(int) / 1e12).astype(int)\n",
    "    \n",
    "    print(f\"  –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –ø–µ—Ä–∏–æ–¥-–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-–¥–µ–π—Å—Ç–≤–∏–µ...\")\n",
    "    \n",
    "    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –ø–µ—Ä–∏–æ–¥—É, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –∏ —Ç–∏–ø—É –¥–µ–π—Å—Ç–≤–∏—è\n",
    "    aggregated = period_actions.groupby(['user_id', 'period', 'action_type_id'], as_index=False).agg(\n",
    "        num_actions=('timestamp', 'nunique'),\n",
    "        timestamp_std=('timestamp_int', 'std'),\n",
    "        num_products=('product_id', 'nunique'),\n",
    "        count_products=('product_id', 'count'),\n",
    "        unique_widget_actions=('widget_name_id', 'nunique'),\n",
    "        num_categories=('category_id', 'nunique'),\n",
    "        category_mode=('category_id', lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 10000),\n",
    "        \n",
    "        price_mean=('price', 'mean'),\n",
    "        price_max=('price', 'max'),\n",
    "        price_min=('price', 'min'),\n",
    "        \n",
    "        discount_price_mean=('discount_price', 'mean'),\n",
    "        discount_price_max=('discount_price', 'max'),\n",
    "        discount_price_min=('discount_price', 'min'),\n",
    "    )\n",
    "    \n",
    "    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ñ–∏—á–∏ –¥–ª—è –ø–µ—Ä–∏–æ–¥–∞ 4 (–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞)\n",
    "    features_to_normalize = [\n",
    "        'num_actions', 'num_products', 'count_products', \n",
    "        'unique_widget_actions', 'num_categories', 'timestamp_std'\n",
    "    ]\n",
    "    \n",
    "    divisor = (end_date - pd.Timedelta(f\"{num_periods*7} days\") - start_date).days\n",
    "    if divisor > 0:\n",
    "        aggregated.loc[aggregated['period'] == num_periods, features_to_normalize] = (\n",
    "            aggregated.loc[aggregated['period'] == num_periods, features_to_normalize] / divisor\n",
    "        )\n",
    "    \n",
    "    print(f\"  –ü–∏–≤–æ—Ç–∏–Ω–≥...\")\n",
    "    \n",
    "    # Pivot –≤ —à–∏—Ä–æ–∫–∏–π —Ñ–æ—Ä–º–∞—Ç\n",
    "    features = [\n",
    "        'num_actions', 'num_products', 'count_products', 'unique_widget_actions',\n",
    "        'num_categories', 'category_mode',\n",
    "        'price_mean', 'price_max', 'price_min',\n",
    "        'discount_price_mean', 'discount_price_max', 'discount_price_min',\n",
    "        'timestamp_std'\n",
    "    ]\n",
    "    \n",
    "    aggregated_wide = aggregated.pivot_table(\n",
    "        index='user_id',\n",
    "        columns=['period', 'action_type_id'],\n",
    "        values=features,\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Flatt column names\n",
    "    aggregated_wide.columns = [\n",
    "        f\"{feat}_{period}_{action}\"\n",
    "        for feat, period, action in aggregated_wide.columns\n",
    "    ]\n",
    "    \n",
    "    aggregated_wide = aggregated_wide.reset_index()\n",
    "    \n",
    "    # –ú–µ—Ä–∂–∏–º\n",
    "    df = df.merge(aggregated_wide, on='user_id', how='left')\n",
    "    \n",
    "    # –ó–∞–ø–æ–ª–Ω—è–µ–º nulls –Ω—É–ª—è–º–∏\n",
    "    periodic_cols = [col for col in df.columns if col not in user_df.columns]\n",
    "    df[periodic_cols] = df[periodic_cols].fillna(0)\n",
    "    \n",
    "    new_features = len(df.columns) - len(user_df.columns)\n",
    "    print(f\"  –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {new_features} –ø–µ—Ä–∏–æ–¥–Ω—ã—Ö —Ñ–∏—á–µ–π –¥–ª—è actions\")\n",
    "    \n",
    "    # –¢–∞–∫–∂–µ –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º search_history –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º\n",
    "    print(f\"  –ê–≥—Ä–µ–≥–∞—Ü–∏—è search_history...\")\n",
    "    \n",
    "    period_searches = search_history[\n",
    "        (search_history['timestamp'] >= start_date) &\n",
    "        (search_history['timestamp'] <= end_date) &\n",
    "        (search_history['user_id'].isin(user_df['user_id']))\n",
    "    ].copy()\n",
    "    \n",
    "    if len(period_searches) > 0:\n",
    "        period_searches['period'] = (\n",
    "            (end_date - period_searches['timestamp']).dt.days // 7\n",
    "        ).clip(upper=num_periods)\n",
    "        \n",
    "        period_searches['timestamp_int'] = (\n",
    "            period_searches['timestamp'].astype(int) / 1e12\n",
    "        ).astype(int)\n",
    "        \n",
    "        search_agg = period_searches.groupby(['user_id', 'period', 'action_type_id'], as_index=False).agg(\n",
    "            num_actions=('timestamp', 'nunique'),\n",
    "            timestamp_std_search=('timestamp_int', 'std'),\n",
    "            unique_widget_search=('widget_name_id', 'nunique'),\n",
    "        )\n",
    "        \n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–∏–æ–¥ 4\n",
    "        if divisor > 0:\n",
    "            search_agg.loc[search_agg['period'] == num_periods, ['num_actions', 'timestamp_std_search', 'unique_widget_search']] = (\n",
    "                search_agg.loc[search_agg['period'] == num_periods, ['num_actions', 'timestamp_std_search', 'unique_widget_search']] / divisor\n",
    "            )\n",
    "        \n",
    "        search_wide = search_agg.pivot_table(\n",
    "            index='user_id',\n",
    "            columns=['period', 'action_type_id'],\n",
    "            values=['num_actions', 'unique_widget_search', 'timestamp_std_search'],\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        search_wide.columns = [\n",
    "            f\"{feat}_{period}_{action}\"\n",
    "            for feat, period, action in search_wide.columns\n",
    "        ]\n",
    "        \n",
    "        search_wide = search_wide.reset_index()\n",
    "        df = df.merge(search_wide, on='user_id', how='left')\n",
    "        \n",
    "        search_cols = [col for col in search_wide.columns if col != 'user_id']\n",
    "        df[search_cols] = df[search_cols].fillna(0)\n",
    "        \n",
    "        print(f\"  –î–æ–±–∞–≤–ª–µ–Ω–æ {len(search_cols)} search –ø–µ—Ä–∏–æ–¥–Ω—ã—Ö —Ñ–∏—á–µ–π\")\n",
    "    \n",
    "    total_new = len(df.columns) - len(user_df.columns)\n",
    "    print(f\"  –í—Å–µ–≥–æ –ø–µ—Ä–∏–æ–¥–Ω—ã—Ö —Ñ–∏—á–µ–π: {total_new}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Features for Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATING TRAINING FEATURES\n",
      "============================================================\n",
      "\n",
      "=== Generating Basic RFM Features ===\n",
      "  Processing clicks...\n",
      "  Processing favorites...\n",
      "  Processing orders...\n",
      "  Processing to_carts...\n",
      "  Processing searches...\n",
      "  Generated 27 RFM features\n",
      "\n",
      "=== Generating Temporal Features ===\n",
      "  Processing click temporal patterns...\n",
      "  Processing favorite temporal patterns...\n",
      "  Processing order temporal patterns...\n",
      "  Processing to_cart temporal patterns...\n",
      "  Generated temporal features\n",
      "\n",
      "=== Generating Conversion Features ===\n",
      "  Generated conversion features\n",
      "\n",
      "=== Generating Advanced Features ===\n",
      "  Calculating discount ratios...\n",
      "  Calculating category diversity...\n",
      "  Calculating widget diversity...\n",
      "  Generated advanced features\n",
      "\n",
      "=== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–∏–æ–¥–Ω—ã—Ö –∞–≥—Ä–µ–≥–∞—Ü–∏–π ===\n",
      "  –ü–µ—Ä–∏–æ–¥—ã: 4 –Ω–µ–¥–µ–ª—å + —Å—Ç–∞—Ä—à–µ\n",
      "  –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –ø–µ—Ä–∏–æ–¥-–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-–¥–µ–π—Å—Ç–≤–∏–µ...\n",
      "  –ü–∏–≤–æ—Ç–∏–Ω–≥...\n",
      "  –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ 260 –ø–µ—Ä–∏–æ–¥–Ω—ã—Ö —Ñ–∏—á–µ–π –¥–ª—è actions\n",
      "  –ê–≥—Ä–µ–≥–∞—Ü–∏—è search_history...\n",
      "  –î–æ–±–∞–≤–ª–µ–Ω–æ 15 search –ø–µ—Ä–∏–æ–¥–Ω—ã—Ö —Ñ–∏—á–µ–π\n",
      "  –í—Å–µ–≥–æ –ø–µ—Ä–∏–æ–¥–Ω—ã—Ö —Ñ–∏—á–µ–π: 275\n",
      "\n",
      "============================================================\n",
      "TOTAL FEATURES GENERATED: 335\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"GENERATING TRAINING FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Start with users and target\n",
    "df_train = val_target.copy()\n",
    "\n",
    "# 1. Basic RFM features\n",
    "df_train = generate_basic_rfm_features(\n",
    "    df_train,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=TRAIN_END_DATE\n",
    ")\n",
    "\n",
    "# 2. Temporal features\n",
    "df_train = generate_temporal_features(\n",
    "    df_train,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=TRAIN_END_DATE\n",
    ")\n",
    "\n",
    "# 3. Conversion features\n",
    "df_train = generate_conversion_features(df_train)\n",
    "\n",
    "# 4. Advanced features\n",
    "df_train = generate_advanced_features(\n",
    "    df_train,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=TRAIN_END_DATE\n",
    ")\n",
    "\n",
    "df_train = generate_periodic_aggregations(\n",
    "    df_train,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=TRAIN_END_DATE,\n",
    "    num_periods=4\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"TOTAL FEATURES GENERATED: {len(df_train.columns) - 2}\")  # -2 for user_id and target\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Summary:\n",
      "Total columns: 337\n",
      "Total rows: 1,835,147\n",
      "\n",
      "First 20 columns:\n",
      "['user_id', 'target', 'num_products_click', 'num_unique_products_click', 'sum_discount_price_click', 'max_discount_price_click', 'days_since_last_click', 'days_since_first_click', 'num_products_favorite', 'num_unique_products_favorite', 'sum_discount_price_favorite', 'max_discount_price_favorite', 'days_since_last_favorite', 'days_since_first_favorite', 'num_products_order', 'num_unique_products_order', 'sum_discount_price_order', 'max_discount_price_order', 'days_since_last_order', 'days_since_first_order']\n",
      "\n",
      "Columns with nulls (>0%):\n",
      "  favorite_to_order_conversion: 90.42%\n",
      "  max_discount_price_favorite: 83.67%\n",
      "  days_since_last_favorite: 83.16%\n",
      "  sum_discount_price_favorite: 83.16%\n",
      "  num_unique_days_favorite: 83.16%\n",
      "  is_new_user_favorite: 83.16%\n",
      "  lifetime_favorite: 83.16%\n",
      "  days_since_first_favorite: 83.16%\n",
      "  favorite_day_of_week_favorite: 83.16%\n",
      "  avg_hour_favorite: 83.16%\n",
      "  num_unique_products_favorite: 83.16%\n",
      "  num_products_favorite: 83.16%\n",
      "  favorite_per_day: 83.16%\n",
      "  searches_to_order_ratio: 74.17%\n",
      "  click_to_order_conversion: 73.65%\n",
      "  to_cart_to_order_conversion: 73.37%\n",
      "  max_discount_price_order: 72.25%\n",
      "  avg_order_price: 72.25%\n",
      "  order_per_day: 72.23%\n",
      "  is_new_user_order: 72.23%\n"
     ]
    }
   ],
   "source": [
    "# Show feature summary\n",
    "print(\"\\nFeature Summary:\")\n",
    "print(f\"Total columns: {len(df_train.columns)}\")\n",
    "print(f\"Total rows: {df_train.shape[0]:,}\")\n",
    "print(f\"\\nFirst 20 columns:\")\n",
    "print(df_train.columns[:20].tolist())\n",
    "\n",
    "# Check for nulls\n",
    "print(f\"\\nColumns with nulls (>0%):\")\n",
    "null_pcts = (df_train.isnull().sum() / len(df_train) * 100).sort_values(ascending=False)\n",
    "null_cols = null_pcts[null_pcts > 0]\n",
    "if len(null_cols) > 0:\n",
    "    for col, pct in null_cols.head(20).items():\n",
    "        print(f\"  {col}: {pct:.2f}%\")\n",
    "else:\n",
    "    print(\"  No null values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Features for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATING TEST FEATURES\n",
      "============================================================\n",
      "\n",
      "=== Generating Basic RFM Features ===\n",
      "  Processing clicks...\n",
      "  Processing favorites...\n",
      "  Processing orders...\n",
      "  Processing to_carts...\n",
      "  Processing searches...\n",
      "  Generated 27 RFM features\n",
      "\n",
      "=== Generating Temporal Features ===\n",
      "  Processing click temporal patterns...\n",
      "  Processing favorite temporal patterns...\n",
      "  Processing order temporal patterns...\n",
      "  Processing to_cart temporal patterns...\n",
      "  Generated temporal features\n",
      "\n",
      "=== Generating Conversion Features ===\n",
      "  Generated conversion features\n",
      "\n",
      "=== Generating Advanced Features ===\n",
      "  Calculating discount ratios...\n",
      "  Calculating category diversity...\n",
      "  Calculating widget diversity...\n",
      "  Generated advanced features\n",
      "\n",
      "=== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–∏–æ–¥–Ω—ã—Ö –∞–≥—Ä–µ–≥–∞—Ü–∏–π ===\n",
      "  –ü–µ—Ä–∏–æ–¥—ã: 4 –Ω–µ–¥–µ–ª—å + —Å—Ç–∞—Ä—à–µ\n",
      "  –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –ø–µ—Ä–∏–æ–¥-–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-–¥–µ–π—Å—Ç–≤–∏–µ...\n",
      "  –ü–∏–≤–æ—Ç–∏–Ω–≥...\n",
      "  –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ 260 –ø–µ—Ä–∏–æ–¥–Ω—ã—Ö —Ñ–∏—á–µ–π –¥–ª—è actions\n",
      "  –ê–≥—Ä–µ–≥–∞—Ü–∏—è search_history...\n",
      "  –î–æ–±–∞–≤–ª–µ–Ω–æ 15 search –ø–µ—Ä–∏–æ–¥–Ω—ã—Ö —Ñ–∏—á–µ–π\n",
      "  –í—Å–µ–≥–æ –ø–µ—Ä–∏–æ–¥–Ω—ã—Ö —Ñ–∏—á–µ–π: 275\n",
      "\n",
      "============================================================\n",
      "TEST FEATURES GENERATED: 335\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"GENERATING TEST FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Start with test users\n",
    "df_test = test_users.copy()\n",
    "df_test['target'] = 0  # Dummy target for consistency\n",
    "\n",
    "# 1. Basic RFM features (using data up to VAL_END_DATE)\n",
    "df_test = generate_basic_rfm_features(\n",
    "    df_test,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=VAL_END_DATE\n",
    ")\n",
    "\n",
    "# 2. Temporal features\n",
    "df_test = generate_temporal_features(\n",
    "    df_test,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=VAL_END_DATE\n",
    ")\n",
    "\n",
    "# 3. Conversion features\n",
    "df_test = generate_conversion_features(df_test)\n",
    "\n",
    "# 4. Advanced features\n",
    "df_test = generate_advanced_features(\n",
    "    df_test,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=VAL_END_DATE\n",
    ")\n",
    "\n",
    "df_test = generate_periodic_aggregations(\n",
    "    df_test,\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=VAL_END_DATE,\n",
    "    num_periods=4\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"TEST FEATURES GENERATED: {len(df_test.columns) - 2}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Selection and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features before cleaning: 335\n",
      "\n",
      "Filling null values with -1...\n",
      "Null values filled.\n"
     ]
    }
   ],
   "source": [
    "# Get feature columns (exclude user_id and target)\n",
    "feature_cols = [col for col in df_train.columns if col not in ['user_id', 'target']]\n",
    "print(f\"Total features before cleaning: {len(feature_cols)}\")\n",
    "\n",
    "# Fill nulls with -1 (indicator for missing)\n",
    "print(\"\\nFilling null values with -1...\")\n",
    "df_train[feature_cols] = df_train[feature_cols].fillna(-1)\n",
    "df_test[feature_cols] = df_test[feature_cols].fillna(-1)\n",
    "\n",
    "print(\"Null values filled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for infinite values...\n",
      "Infinite values handled.\n"
     ]
    }
   ],
   "source": [
    "# Check for infinite values\n",
    "print(\"\\nChecking for infinite values...\")\n",
    "\n",
    "# Replace inf with very large number\n",
    "df_train = df_train.replace([np.inf, -np.inf], 999999)\n",
    "df_test = df_test.replace([np.inf, -np.inf], 999999)\n",
    "\n",
    "print(\"Infinite values handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Statistics:\n",
      "Total features: 335\n",
      "Train shape: (1835147, 337)\n",
      "Test shape: (2068424, 337)\n",
      "\n",
      "Numeric features: 335\n"
     ]
    }
   ],
   "source": [
    "# Basic feature statistics\n",
    "print(\"\\nFeature Statistics:\")\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "# Feature types\n",
    "numeric_features = df_train[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nNumeric features: {len(numeric_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving features...\n",
      "\n",
      "Features saved to ../results/\n",
      "  - features_train.parquet: (1835147, 337)\n",
      "  - features_test.parquet: (2068424, 337)\n"
     ]
    }
   ],
   "source": [
    "# Save as parquet (compressed, fast)\n",
    "output_dir = '../results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Saving features...\")\n",
    "\n",
    "df_train.to_parquet(os.path.join(output_dir, 'features_train_v2.parquet'), index=False)\n",
    "df_test.to_parquet(os.path.join(output_dir, 'features_test_v2.parquet'), index=False)\n",
    "\n",
    "print(f\"\\nFeatures saved to {output_dir}/\")\n",
    "print(f\"  - features_train.parquet: {df_train.shape}\")\n",
    "print(f\"  - features_test.parquet: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature names saved to ../results/feature_names_v2.txt\n",
      "Total: 335 features\n"
     ]
    }
   ],
   "source": [
    "# Save feature names\n",
    "with open(os.path.join(output_dir, 'feature_names_v2.txt'), 'w') as f:\n",
    "    for col in feature_cols:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "print(f\"\\nFeature names saved to {output_dir}/feature_names_v2.txt\")\n",
    "print(f\"Total: {len(feature_cols)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "üìä Summary:\n",
      "  Total features generated: 335\n",
      "  Training samples: 1,835,147\n",
      "  Test samples: 2,068,424\n",
      "  Positive class ratio: 34.59%\n",
      "\n",
      "‚úÖ Feature Categories:\n",
      "  - Basic RFM features (Recency, Frequency, Monetary)\n",
      "  - Temporal patterns (day of week, hour, activity consistency)\n",
      "  - Conversion features (funnel metrics)\n",
      "  - Advanced features (diversity, price sensitivity)\n",
      "\n",
      "üìÅ Output Files:\n",
      "  - ../results/features_train.parquet\n",
      "  - ../results/features_test.parquet\n",
      "  - ../results/feature_names.txt\n",
      "\n",
      "üéØ Next Steps:\n",
      "  1. Review feature distributions in EDA\n",
      "  2. Train models (03_modeling.ipynb)\n",
      "  3. Analyze feature importance\n",
      "  4. Consider adding UMAP embeddings or periodic aggregations\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"  Total features generated: {len(feature_cols)}\")\n",
    "print(f\"  Training samples: {df_train.shape[0]:,}\")\n",
    "print(f\"  Test samples: {df_test.shape[0]:,}\")\n",
    "print(f\"  Positive class ratio: {positive_ratio:.2%}\")\n",
    "\n",
    "print(\"\\n‚úÖ Feature Categories:\")\n",
    "print(\"  - Basic RFM features (Recency, Frequency, Monetary)\")\n",
    "print(\"  - Temporal patterns (day of week, hour, activity consistency)\")\n",
    "print(\"  - Conversion features (funnel metrics)\")\n",
    "print(\"  - Advanced features (diversity, price sensitivity)\")\n",
    "\n",
    "print(\"\\nüìÅ Output Files:\")\n",
    "print(f\"  - {output_dir}/features_train.parquet\")\n",
    "print(f\"  - {output_dir}/features_test.parquet\")\n",
    "print(f\"  - {output_dir}/feature_names.txt\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"  1. Review feature distributions in EDA\")\n",
    "print(\"  2. Train models (03_modeling.ipynb)\")\n",
    "print(\"  3. Analyze feature importance\")\n",
    "print(\"  4. Consider adding UMAP embeddings or periodic aggregations\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of generated features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>target</th>\n",
       "      <th>num_products_click</th>\n",
       "      <th>num_unique_products_click</th>\n",
       "      <th>sum_discount_price_click</th>\n",
       "      <th>max_discount_price_click</th>\n",
       "      <th>days_since_last_click</th>\n",
       "      <th>days_since_first_click</th>\n",
       "      <th>num_products_favorite</th>\n",
       "      <th>num_unique_products_favorite</th>\n",
       "      <th>sum_discount_price_favorite</th>\n",
       "      <th>max_discount_price_favorite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20407.0</td>\n",
       "      <td>17257.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7378.0</td>\n",
       "      <td>647.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>103.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>37495.0</td>\n",
       "      <td>5310.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>318.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>17255.0</td>\n",
       "      <td>3373.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2353.0</td>\n",
       "      <td>1199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  target  num_products_click  num_unique_products_click  \\\n",
       "0       16       0                 1.0                        1.0   \n",
       "1       34       0                -1.0                       -1.0   \n",
       "2       36       1                 9.0                        9.0   \n",
       "3       53       0                -1.0                       -1.0   \n",
       "4       54       0                 1.0                        1.0   \n",
       "5       58       0                 6.0                        6.0   \n",
       "6       62       0                35.0                       30.0   \n",
       "7       64       1               103.0                       77.0   \n",
       "8       83       0                30.0                       29.0   \n",
       "9       90       0                -1.0                       -1.0   \n",
       "\n",
       "   sum_discount_price_click  max_discount_price_click  days_since_last_click  \\\n",
       "0                     335.0                     335.0                  118.0   \n",
       "1                      -1.0                      -1.0                   -1.0   \n",
       "2                   20407.0                   17257.0                   49.0   \n",
       "3                      -1.0                      -1.0                   -1.0   \n",
       "4                     110.0                     110.0                    4.0   \n",
       "5                    1007.0                     389.0                   60.0   \n",
       "6                    7378.0                     647.0                   19.0   \n",
       "7                   37495.0                    5310.0                   10.0   \n",
       "8                   17255.0                    3373.0                   12.0   \n",
       "9                      -1.0                      -1.0                   -1.0   \n",
       "\n",
       "   days_since_first_click  num_products_favorite  \\\n",
       "0                   118.0                   -1.0   \n",
       "1                    -1.0                   -1.0   \n",
       "2                    73.0                   -1.0   \n",
       "3                    -1.0                   -1.0   \n",
       "4                     4.0                   -1.0   \n",
       "5                   116.0                   -1.0   \n",
       "6                    96.0                   -1.0   \n",
       "7                   121.0                    1.0   \n",
       "8                    96.0                    2.0   \n",
       "9                    -1.0                   -1.0   \n",
       "\n",
       "   num_unique_products_favorite  sum_discount_price_favorite  \\\n",
       "0                          -1.0                         -1.0   \n",
       "1                          -1.0                         -1.0   \n",
       "2                          -1.0                         -1.0   \n",
       "3                          -1.0                         -1.0   \n",
       "4                          -1.0                         -1.0   \n",
       "5                          -1.0                         -1.0   \n",
       "6                          -1.0                         -1.0   \n",
       "7                           1.0                        318.0   \n",
       "8                           2.0                       2353.0   \n",
       "9                          -1.0                         -1.0   \n",
       "\n",
       "   max_discount_price_favorite  \n",
       "0                         -1.0  \n",
       "1                         -1.0  \n",
       "2                         -1.0  \n",
       "3                         -1.0  \n",
       "4                         -1.0  \n",
       "5                         -1.0  \n",
       "6                         -1.0  \n",
       "7                        318.0  \n",
       "8                       1199.0  \n",
       "9                         -1.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sample of features\n",
    "print(\"\\nSample of generated features:\")\n",
    "display_cols = ['user_id', 'target'] + feature_cols[:10]\n",
    "df_train[display_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
